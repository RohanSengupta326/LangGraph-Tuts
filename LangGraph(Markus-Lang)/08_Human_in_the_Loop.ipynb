{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The add_messages method executes:\n",
    "\n",
    "During state updates: When a node returns a dictionary with a \"messages\" key, the add_messages method is automatically called to combine the new messages with existing messages in the state.\n",
    "\n",
    "Between node transitions: As the graph flows from one node to another, when state is passed between nodes, the annotations are applied to merge returned values with existing state values.\n",
    "\n",
    "Specifically at merge points: When multiple branches of execution need to be merged (like after parallel tool execution), the annotations determine how those values should be combined.\n",
    "\"\"\"\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \"\"\"Call to get the current weather.\"\"\"\n",
    "    if location.lower() in [\"munich\"]:\n",
    "        return \"It's 15 degrees Celsius and cloudy.\"\n",
    "    else:\n",
    "        return \"It's 32 degrees Celsius and sunny.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def broken_api(location: str):\n",
    "    \"\"\"Call to get the current weather.\"\"\"\n",
    "    return f\"Currently no weather data available for {location}. Please try again later\"\n",
    "\n",
    "\n",
    "tools = [get_weather, broken_api]\n",
    "llm = ChatOllama(\n",
    "        model=\"mistral\",\n",
    "        temperature=0,\n",
    "    )\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "\"\"\" Sets up conditional edges using tools_condition (a prebuilt function that routes to tools if needed) \"\"\"\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "# Adds an edge from tools back to chatbot\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "\"\"\" \n",
    "Importantly, adds interrupt_before=[\"tools\"] which is the key for human intervention \n",
    "When you use interrupt_before=[\"tools\"], you're telling LangGraph to pause execution right before the \"tools\" node would be executed.\n",
    "\"\"\"\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[\"tools\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"Hello, I am John\")\n",
    "# this won't even need to call tools so graph won't be paused.\n",
    "\n",
    "graph.invoke({\"messages\": input_message}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "These lines below:  \n",
    "\n",
    "Create two separate conversations with different thread IDs\n",
    "Show how the same question gets different answers because of conversation memory\n",
    "Use the MemorySaver to maintain state between interactions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(content=\"Sorry, did I already introduce myself?\")\n",
    "# this won't even need to call tools so graph won't be paused.\n",
    "\n",
    "graph.invoke({\"messages\": input_message}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"Sorry, did I already introduce myself?\")\n",
    "# this won't even need to call tools so graph won't be paused.\n",
    "\n",
    "graph.invoke({\"messages\": input_message}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"How is the weather in Munich?\")\n",
    "\n",
    "graph.invoke({\"messages\": input_message}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The system is essentially saying: \"For conversation thread 3, I've paused execution and the next step will be to run the tools node once you tell me to continue.\" \"\"\"\n",
    "\n",
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(None, config=config) #resume\n",
    "\n",
    "\"\"\" \n",
    "This demonstrates:\n",
    "\n",
    "Starting a weather query conversation\n",
    "The graph pausing before executing tools because of interrupt_before=[\"tools\"]\n",
    "Resuming execution with no new input\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timetravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"How is the weather in Munich?\")\n",
    "\n",
    "graph.invoke({\"messages\": input_message}, config=config)\n",
    "\n",
    "\"\"\"\n",
    "This code:\n",
    "\n",
    "Creates a new conversation with thread ID \"4\"\n",
    "Sends a message asking about the weather in Munich\n",
    "Runs the graph with this input\n",
    "\n",
    "Since the graph was compiled with interrupt_before=[\"tools\"] and this weather question will trigger the get_weather tool, the execution will pause right before the tools node runs.\n",
    "At this point, the graph is in a \"paused\" state waiting for further instruction. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "existing_message = snapshot.values[\"messages\"][-1]\n",
    "existing_message.pretty_print()\n",
    "\n",
    "\"\"\"\n",
    "You're doing exactly these three steps:\n",
    "\n",
    "snapshot = graph.get_state(config) - Get the current state of the graph for thread ID \"4\" (which is stored in the config variable)\n",
    "existing_message = snapshot.values[\"messages\"][-1] - Access the last message in the messages list that's stored in the state. The [-1] is Python's way of accessing the last element in a list.\n",
    "existing_message.pretty_print() - Display that message in a readable format\n",
    "\n",
    "At this point, the message you're examining would typically be an AI message that contains a tool call - specifically a call to the weather tool for Munich. This is the message that was generated just before the graph paused execution (because of the interrupt_before=[\"tools\"] setting).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "You're creating two new messages and manually inserting them into the conversation state:\n",
    "\n",
    "ToolMessage: This represents the response from a tool. You're creating a fake response as if the weather tool had returned \"It is only 5Â°C warm today!\" instead of its actual response. The important part is that you're connecting it to the original tool call by using \n",
    "\n",
    "tool_call_id=existing_message.tool_calls[0][\"id\"]\n",
    "\n",
    "which links this response to the specific tool call that was made.\n",
    "\n",
    "AIMessage: This represents the AI's response after receiving the tool's output. You're setting it to the same text, simulating what the AI might say after getting the tool result.\n",
    "\n",
    "Then with graph.update_state(), you're injecting these messages into the conversation, effectively overriding what would have happened with the actual tool execution.\n",
    "\"\"\"\n",
    "\n",
    "answer = \"It is only 5Â°C warm today!\"\n",
    "new_messages = [\n",
    "    ToolMessage(content=answer, tool_call_id=existing_message.tool_calls[0][\"id\"]),\n",
    "    AIMessage(content=answer),\n",
    "]\n",
    "\n",
    "\"\"\" \n",
    "as the last message would the AIMessage with the tool call , in the tool_call_id we are accessing that. \n",
    "but the graph was paused before the tool call node execution. \n",
    "\"\"\"\n",
    "\n",
    "graph.update_state(\n",
    "    config,\n",
    "    {\"messages\": new_messages},\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "due to the use of Annotated , these new_messages will be appended to the previous messages list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((graph.get_state(config).values[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You're starting a new invocation of the graph (not resuming the paused one)\n",
    "Since you're providing a new input message, the graph considers this a fresh conversation turn\n",
    "The graph starts from the beginning (entry point) with this new message\n",
    "But critically, it's using the same thread ID, so it has access to all previous messages ( inlcluding the fake messages you inserted after pausing the graph execution ) \n",
    "\n",
    "The graph doesn't maintain multiple \"paused states\" within the same thread - when you invoke it with a new message, it treats that as a new turn in the conversation and starts from the beginning.\n",
    "\n",
    "The original paused state is essentially abandoned when you start a new conversation turn.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "so won't the graph pause again at the pause point? i.e before executing 'tool' node.\n",
    "\n",
    "The LLM now has a complete conversation including what appears to be a previously completed tool call (your injected messages)\n",
    "When the LLM generates its next response to \"How warm was it again?\", it likely doesn't need to call a tool again - it can simply respond based on the \"temperature information\" that appears in the conversation history\n",
    "\n",
    "Therefore, the graph doesn't hit the pause point because the execution path doesn't lead to tool use. The LLM is responding directly with something like \"It was 5Â°C\" based on the conversation history.\n",
    "\n",
    "If the follow-up question had required a new tool call, then yes, the graph would have paused again before executing that new tool.\n",
    "and we would have to invoke with None type input with same thread_id to resume the next node execution. \n",
    "\"\"\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"How warm was it again?\")\n",
    "\n",
    "graph.invoke({\"messages\": input_message}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpoints = []\n",
    "for state in graph.get_state_history(config=config):\n",
    "    # get all the history of states of that thread_id.\n",
    "    all_checkpoints.append(state)\n",
    "all_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When you select a specific checkpoint with to_replay = all_checkpoints[4], you're selecting both:\n",
    "\n",
    "The state data as it existed at that historical moment\n",
    "The execution position in the graph at that historical moment\n",
    "\"\"\"\n",
    "\n",
    "to_replay = all_checkpoints[4]\n",
    "\n",
    "to_replay.values #Displays the values in that checkpoint (like messages, state variables, etc.)\n",
    "#The .values attribute contains all the actual data in that particular state snapshot, similar to how state data is accessed in various nodes across your notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replay.next #Shows what node would be executed next if you resumed from this checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replay.config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When you use graph.invoke(None, config=to_replay.config), you're resuming execution from the exact node position that's stored in checkpoint 4 - regardless of whether the graph was explicitly paused there or not.\n",
    "Here's how it works:\n",
    "\n",
    "If the graph was paused at that position (due to interrupt_before or for any other reason), resuming will continue execution from that paused point.\n",
    "If the graph was not explicitly paused at that position, but that's just where execution was at the moment the checkpoint was recorded, the system will still resume from that exact node. The checkpoint records the precise state of execution, including which node was about to be executed next.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "But if we paused the graph before tool_execution, and haven't resumed it, \n",
    "so invoking below from any state, will replay the graph from that historical state and node \n",
    "but will \n",
    "again come and pause where it was paused before, i.e before the tool execution, as the graph hasn't been resumed yet.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "When you access a checkpoint from the exact point where the graph was paused (before tool execution) and invoke with `None`, here's what happens:\n",
    "\n",
    "If you do `to_replay = all_checkpoints[4]` and that checkpoint represents the state where the graph was paused before tool execution, then when you call `graph.invoke(None, config=to_replay.config)`:\n",
    "\n",
    "1. The system will recognize that this state was at a pause point (before tools execution)\n",
    "2. It will resume execution from that pause point\n",
    "3. It will continue until the next pause point or until completion\n",
    "\n",
    "It doesn't \"double pause\" or stay paused - it resumes execution from the pause point and continues running the graph. The `None` input explicitly tells the system \"don't add new input, just continue execution from where this state was.\"\n",
    "\n",
    "This is different from accessing the current state with `graph.get_state(config)` which would tell you the graph is currently paused but wouldn't resume execution.\n",
    "\"\"\"\n",
    "\n",
    "graph.invoke(None, config=to_replay.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branching off past state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from that accessed index 4 state, we are finding out the last message from the state.\n",
    "\"\"\"\n",
    "\n",
    "last_message = to_replay.values[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This modifies the tool call to change which tool will be used\n",
    "It changes the name from \"get_weather\" to \"broken_api\"\n",
    "This is the key line that creates the branch - we're altering history to see what would happen if a different tool had been called\n",
    "\"\"\"\n",
    "last_message.tool_calls[0][\"name\"] = \"broken_api\"\n",
    "\n",
    "\"\"\" This displays the modified tool calls to confirm our change worked\n",
    "We can see that the name is now \"broken_api\" instead of the original value \"\"\"\n",
    "last_message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This updates the state using our modified message\n",
    "to_replay.config tells it which thread/conversation to update\n",
    "{\"messages\": [last_message]} provides the modified message to insert\n",
    "The return value branch_config is a new configuration that points to this new branched state\n",
    "This is creating an alternative timeline based on our modification\n",
    "\"\"\"\n",
    "branch_config = graph.update_state(\n",
    "    to_replay.config,\n",
    "    {\"messages\": [last_message]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This continues execution using our newly created branch\n",
    "None means no new input, just resume execution\n",
    "branch_config points to our modified state that uses \"broken_api\" instead\n",
    "This will show the conversation continuing as if the AI had originally called the broken API\n",
    "\"\"\"\n",
    "graph.invoke(None, branch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This returns to the original conversation thread by using the original config\n",
    "It shows that we can explore branches but still return to the main conversation\n",
    "This demonstrates that branches don't overwrite the original conversation history\n",
    "\"\"\"\n",
    "graph.invoke(None, config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    ask_human: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\"\"\"\n",
    "This defines a special tool that:\n",
    "\n",
    "Allows the AI to explicitly request human assistance\n",
    "Returns an empty string since the actual response will come from the human\n",
    "The docstring explains to the AI when to use this tool\n",
    "\"\"\"\n",
    "@tool\n",
    "def request_assistance():\n",
    "    \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
    "\n",
    "    To use this function, relay the user's 'request' so the expert can provide the right guidance.\n",
    "    \"\"\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather]\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\"\"\"\n",
    "Note that we're adding request_assistance separately, showing how to combine regular tools with special human-intervention tools\n",
    "\"\"\"\n",
    "llm_with_tools = llm.bind_tools(tools + [request_assistance])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Gets the LLM's response based on the message history\n",
    "Sets a flag ask_human to True if the LLM called the request_assistance tool\n",
    "Returns both the response and the flag\n",
    "\"\"\"\n",
    "def chatbot(state: State):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    ask_human = False\n",
    "    if response.tool_calls and response.tool_calls[0][\"name\"] == \"request_assistance\":\n",
    "        ask_human = True\n",
    "\n",
    "\n",
    "    state[\"ask_human\"] = True\n",
    "    state['messages'] = response\n",
    "    return state\n",
    "\n",
    "    \"\"\" this approach below returns a new state not update the existing one \"\"\"\n",
    "    # return {\"messages\": [response], \"ask_human\": ask_human}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This section:\n",
    "\n",
    "Defines a helper function create_response that formats the human's response as a tool response\n",
    "Creates a human_node function that:\n",
    "\n",
    "Checks if there's already a tool response\n",
    "If not, creates a new ToolMessage with expert advice\n",
    "Resets the ask_human flag to false\n",
    "This simulates a human expert providing a specialized response\n",
    "\n",
    "\n",
    "Adds the human node to the graph\n",
    "\"\"\"\n",
    "def create_response(response: str, ai_message: AIMessage):\n",
    "    return ToolMessage(\n",
    "        content=response,\n",
    "        tool_call_id=ai_message.tool_calls[0][\"id\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    new_messages = []\n",
    "    ''' \n",
    "    meaning if aleady no tool has been called and ToolMessage was added to the message list.\n",
    "    only then append.\n",
    "    '''\n",
    "    if not isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        new_messages.append(\n",
    "            create_response(\n",
    "                \"Plan your trip 3 months before and you don't carry a Real Madrid shirt with you\",\n",
    "                state[\"messages\"][-1],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    state[\"messages\"] = new_messages\n",
    "    state[\"ask_human\"] = False\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"human\", human_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This:\n",
    "\n",
    "Creates a router function that:\n",
    "\n",
    "Checks if human intervention is needed\n",
    "If yes, routes to the human node\n",
    "If no, uses the standard tools_condition to decide\n",
    "\n",
    "\n",
    "Sets up conditional edges with three possible destinations:\n",
    "\n",
    "\"human\" node if human intervention is needed\n",
    "\"tools\" node if a regular tool call is made\n",
    "END if no tools or human intervention needed\n",
    "\"\"\"\n",
    "\n",
    "def select_next_node(state: State):\n",
    "    if state[\"ask_human\"]:\n",
    "        return \"human\"\n",
    "    return tools_condition(state)\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    select_next_node,\n",
    "    {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This:\n",
    "\n",
    "Adds edges from both tools and human nodes back to chatbot\n",
    "Sets the entry point to chatbot\n",
    "Creates a memory saver for conversation history\n",
    "Compiles the graph with an important addition: interrupt_before=[\"human\"]\n",
    "The interrupt_before=[\"human\"] is crucial - it pauses execution before the human node runs\n",
    "This allows a real human to intervene in the workflow\n",
    "\"\"\"\n",
    "\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(\"human\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "checkpointer = MemorySaver()\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[\"human\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This:\n",
    "\n",
    "Sets up a configuration with a thread ID\n",
    "Creates a message asking for travel advice\n",
    "Invokes the graph with this message\n",
    "When the AI detects this needs expertise, it will call request_assistance\n",
    "The graph will pause before the human node\n",
    "\"\"\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"50\"}}\n",
    "input_message = HumanMessage(\n",
    "    content=\"I need some expert advice on how to plan a trip to barcelona\"\n",
    ")\n",
    "graph.invoke({\"messages\": input_message}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This:\n",
    "\n",
    "Resumes execution after the human has reviewed : (in this case it will just answers with a hardcoded human response and convert it to a ToolMessage type, in the human_node )\n",
    "Continues the conversation flow\n",
    "Demonstrates how a human can be integrated into the conversation flow\n",
    "\"\"\"\n",
    "graph.invoke(None, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New way Human in the loop - Interrupt + Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from typing_extensions import Literal\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    string_value: str\n",
    "    numeric_value: int\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A helper function that modifies the state by adding \"a\" to strings and 1 to numbers\n",
    "Used by the node functions below\n",
    "\"\"\"\n",
    "def modify_state(input_state: InputState) -> InputState:\n",
    "    input_state[\"string_value\"] += \"a\"\n",
    "    input_state[\"numeric_value\"] += 1\n",
    "    return input_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Unlike earlier patterns, this returns a Command object\n",
    "The Command contains:\n",
    "\n",
    "goto=\"branch_b\": Where to go next\n",
    "update=new_state: State changes to apply\n",
    "\n",
    "\n",
    "This explicitly controls both flow and state in one return value\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "When you use Command in return types, you're implementing what's called \"imperative routing\" - you're directly specifying where to go next. \n",
    "\n",
    "This implements an edgeless graph. as the Command can tell where to go and update states.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Here's what the Literal[\"branch_b\"] means and why it's significant:\n",
    "\n",
    "Type Safety and Validation: It declares that this function can only return a Command that routes to \"branch_b\" and no other destination. This acts as a compile-time check to ensure the node only navigates to allowed destinations.\n",
    "\n",
    "Avoiding Runtime Errors: It helps prevent errors where a node might try to navigate to a destination that doesn't exist or isn't connected in the graph.\n",
    "\n",
    "The Literal type ensures that the dynamic routing done with Command still follows the statically defined structure of your graph, combining the flexibility of runtime decisions with the safety of compile-time checks.\n",
    "\"\"\"\n",
    "def branch_a(state: InputState) -> Command[Literal[\"branch_b\"]]:\n",
    "    print(f\"branch_a: Current state: {state}\")\n",
    "\n",
    "    \"\"\"\n",
    "    Since the Command API is designed to work with new states, emitting a new state (update=new_state) is the correct approach here.\n",
    "    Updating the existing state directly would bypass the Command mechanism and could lead to unexpected behavior.\n",
    "    \"\"\"\n",
    "    new_state = modify_state(state)\n",
    "    print(f\"branch_a: Updated state: {new_state}\")\n",
    "    return Command(\n",
    "        goto=\"branch_b\",\n",
    "        update=new_state,\n",
    "    )\n",
    "\n",
    "\n",
    "def branch_b(state: InputState) -> Command[Literal[\"branch_c\"]]:\n",
    "    print(f\"branch_b: Current state: {state}\")\n",
    "    new_state = modify_state(state)\n",
    "    print(f\"branch_b: Updated state: {new_state}\")\n",
    "    return Command(\n",
    "        goto=\"branch_c\",\n",
    "        update=new_state,\n",
    "    )\n",
    "\n",
    "\n",
    "def branch_c(state: InputState) -> Command[Literal[END]]:\n",
    "    print(f\"branch_c: Current state: {state}\")\n",
    "    new_state = modify_state(state)\n",
    "    print(f\"branch_c: Updated state: {new_state}\")\n",
    "    return Command(goto=END, update=new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(InputState)\n",
    "\n",
    "graph.add_node(\"branch_a\", branch_a)\n",
    "graph.add_node(\"branch_b\", branch_b)\n",
    "graph.add_node(\"branch_c\", branch_c)\n",
    "graph.set_entry_point(\"branch_a\")\n",
    "\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\"string_value\": \"Hello\", \"numeric_value\": 0}\n",
    "final_state = runnable.invoke(initial_state)\n",
    "\n",
    "print(\"Final state:\", final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Command, interrupt\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "@tool\n",
    "def weather_search(city: str):\n",
    "    \"\"\"Search for the weather in a specific city\"\"\"\n",
    "    print(\"----\")\n",
    "    print(f\"Searching for: {city}\")\n",
    "    print(\"----\")\n",
    "    return \"Sunny!\"\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools([weather_search])\n",
    "\n",
    "\n",
    "def call_llm(state):\n",
    "    return {\"messages\": [model.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "\n",
    "def human_review_node(state) -> Command[Literal[\"call_llm\", \"run_tool\"]]:\n",
    "    \"\"\"\n",
    "    Gets the last message and its tool call\n",
    "    Key new feature: Uses the interrupt() function to pause execution\n",
    "    Provides context for the human reviewer (the question and tool call)\n",
    "    This actively pauses execution and waits for human input\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_call = last_message.tool_calls[-1]\n",
    "\n",
    "    human_review = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            \"tool_call\": tool_call,\n",
    "        }\n",
    "    )\n",
    "    print(\"Human review values:\", human_review)\n",
    "\n",
    "    \"\"\"\n",
    "    Gets the human's decision (continue or update)\n",
    "    If \"continue\", uses Command to direct flow to run_tool without changes\n",
    "    \"\"\"\n",
    "    review_action = human_review[\"action\"]\n",
    "    review_data = human_review.get(\"data\")\n",
    "\n",
    "    if review_action == \"continue\":\n",
    "        return Command(goto=\"run_tool\")\n",
    "\n",
    "\n",
    "    elif review_action == \"update\":\n",
    "        \"\"\"\n",
    "        If \"update\", creates a modified message with the human's corrections\n",
    "        Returns a Command that both:\n",
    "\n",
    "        Routes to the run_tool node\n",
    "        Updates the state with the corrected message\n",
    "\n",
    "\n",
    "        This allows humans to modify parameters before tool execution\n",
    "        \"\"\"\n",
    "        updated_message = {\n",
    "            \"role\": \"ai\",\n",
    "            \"content\": last_message.content,\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": tool_call[\"id\"],\n",
    "                    \"name\": tool_call[\"name\"],\n",
    "                    \"args\": review_data,\n",
    "                }\n",
    "            ],\n",
    "            \"id\": last_message.id,\n",
    "        }\n",
    "        return Command(goto=\"run_tool\", update={\"messages\": [updated_message]})\n",
    "\n",
    "\n",
    "def route_after_llm(state) -> Literal[END, \"human_review_node\"]:\n",
    "    if len(state[\"messages\"][-1].tool_calls) == 0:\n",
    "        return END\n",
    "    else:\n",
    "        return \"human_review_node\"\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_llm\", call_llm)\n",
    "builder.add_node(\"run_tool\", ToolNode(tools=[weather_search]))\n",
    "builder.add_node(\"human_review_node\", human_review_node)\n",
    "\n",
    "\"\"\"\n",
    "only adding edges where return type is not Command, as Command type can decide where to go itself, we don't need edges. \n",
    "as you can see in the human_review_node.\n",
    "\"\"\"\n",
    "builder.add_conditional_edges(\"call_llm\", route_after_llm)\n",
    "builder.add_edge(\"run_tool\", \"call_llm\")\n",
    "builder.set_entry_point(\"call_llm\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Starts a conversation about weather\n",
    "Note the typo in \"Munic\" (not \"Munich\")\n",
    "\"\"\"\n",
    "initial_input = {\"messages\": [HumanMessage(content=\"How is the weather in Munic?\")]}\n",
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "\n",
    "first_result = graph.invoke(initial_input, config=config, stream_mode=\"updates\")\n",
    "first_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Shows that execution is waiting at the human review node\n",
    "print(graph.get_state(config).next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resumes execution, choosing to continue without changes\n",
    "The weather search will run with \"Munic\" (incorrect)\n",
    "\"\"\"\n",
    "graph.invoke(Command(resume={\"action\": \"continue\"}), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Starts a new conversation with the same typo with a new thread_id.\n",
    "But this time we'll correct it\n",
    "\"\"\"\n",
    "config = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "\n",
    "graph.invoke(initial_input, config=config, stream_mode=\"updates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.get_state(config).next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Resumes execution but with corrected data\n",
    "This shows how humans can fix errors before tool execution\n",
    "The Command with resume provides structured data for the correction\n",
    "\"\"\"\n",
    "graph.invoke(Command(resume={\"action\": \"update\", \"data\": {\"city\": \"Munich\"}}), config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangGraph_Markus-Lang_-Rz6FeIEr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
