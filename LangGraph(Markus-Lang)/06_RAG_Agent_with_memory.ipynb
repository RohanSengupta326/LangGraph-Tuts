{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Bella Vista is owned by Antonio Rossi, a renowned chef with over 20 years of experience in the culinary industry. He started Bella Vista to bring authentic Italian flavors to the community.\",\n",
    "        metadata={\"source\": \"owner.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Bella Vista offers a range of dishes with prices that cater to various budgets. Appetizers start at $8, main courses range from $15 to $35, and desserts are priced between $6 and $12.\",\n",
    "        metadata={\"source\": \"dishes.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Bella Vista is open from Monday to Sunday. Weekday hours are 11:00 AM to 10:00 PM, while weekend hours are extended from 11:00 AM to 11:00 PM.\",\n",
    "        metadata={\"source\": \"restaurant_info.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Bella Vista offers a variety of menus including a lunch menu, dinner menu, and a special weekend brunch menu. The lunch menu features light Italian fare, the dinner menu offers a more extensive selection of traditional and contemporary dishes, and the brunch menu includes both classic breakfast items and Italian specialties.\",\n",
    "        metadata={\"source\": \"restaurant_info.txt\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "# retriever = db.as_retriever()\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the following context and the Chat history. Especially take the latest question into consideration:\n",
    "\n",
    "Chathistory: {history}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "rag_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "\"\"\" creating the state ( data storage in simple term to be passed from node to node), nodes and routers. \"\"\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    documents: List[Document]\n",
    "    on_topic: str\n",
    "    rephrased_question: str\n",
    "    proceed_to_generate: bool\n",
    "    rephrase_count: int\n",
    "    question: HumanMessage\n",
    "\n",
    "\n",
    "class GradeQuestion(BaseModel):\n",
    "    score: str = Field(\n",
    "        description=\"Question is about the specified topics? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Purpose: If the user's question depends on past messages, rewrite it to be standalone. \n",
    "e.g : \n",
    "Before: \"Also on Sunday?\"\n",
    "After: \"Is Bella Vista restaurant open on Sundays?\"\n",
    "\"\"\"\n",
    "def question_rewriter(state: AgentState):\n",
    "    print(f\"Entering question_rewriter with following state: {state}\")\n",
    "\n",
    "    # Reset state variables except for 'question' and 'messages'\n",
    "    state[\"documents\"] = []\n",
    "    state[\"on_topic\"] = \"\"\n",
    "    state[\"rephrased_question\"] = \"\"\n",
    "    state[\"proceed_to_generate\"] = False\n",
    "    state[\"rephrase_count\"] = 0\n",
    "\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "\n",
    "    if state[\"question\"] not in state[\"messages\"]:\n",
    "        state[\"messages\"].append(state[\"question\"])\n",
    "\n",
    "    \"\"\" \n",
    "    if conversation is >  1 message then we'll take the conversation context to create\n",
    "    standalone question. else the questions itself is the rephrased question. \n",
    "    \"\"\"\n",
    "    if len(state[\"messages\"]) > 1:\n",
    "        \"\"\" slicing a list stored inside state[\"messages\"] and excluding the last element. \"\"\"\n",
    "        conversation = state[\"messages\"][:-1]\n",
    "        current_question = state[\"question\"].content\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant that rephrases the user's question to be a standalone question optimized for retrieval.\"\n",
    "            )\n",
    "        ]\n",
    "        \"\"\" .extend(conversation) → Adds all elements from conversation to messages. \"\"\"\n",
    "        messages.extend(conversation)\n",
    "        messages.append(HumanMessage(content=current_question))\n",
    "        \n",
    "        rephrase_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        \"\"\" \n",
    "        using pipe operator is better rather than the below invoking method. \n",
    "        chain = rephrase_prompt | llm\n",
    "        response = chain.invoke({}) \n",
    "        \"\"\"\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        prompt = rephrase_prompt.format() #format for llm understanding into str\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "\n",
    "        \"\"\" ALSO COULD HAVE DONE SIMILARLY like previous implementations \"\"\"\n",
    "        \"\"\" \n",
    "        template = '''\n",
    "        Rephrase the user's question to be a standalone query that is self-contained.\n",
    "        If conversation history exists, use it to improve the rephrasing.\n",
    "        \n",
    "        Conversation History: {context}\n",
    "        Question: {question}\n",
    "        \n",
    "        Rephrased Question:\n",
    "        '''\n",
    "\n",
    "        # Create a PromptTemplate object\n",
    "        rephrase_prompt = PromptTemplate.from_template(template=template)\n",
    "\n",
    "        # would have to convert to normal str cause the message in state is actually a list of basemessages\n",
    "        # or, simply the message property of state to a different type which directly supports conversation.\n",
    "        conversation_text = \"\\n\".join(msg.content for msg in conversation)\n",
    "\n",
    "        # Define the LLM model\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "        # Use the pipe (`|`) operator to chain prompt → LLM → invoke\n",
    "        chain = rephrase_prompt | llm\n",
    "        response = chain.invoke({\"context\": conversation_text, \"question\": current_question})\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        better_question = response.content.strip()\n",
    "        print(f\"question_rewriter: Rephrased question: {better_question}\")\n",
    "        state[\"rephrased_question\"] = better_question\n",
    "    else:\n",
    "        state[\"rephrased_question\"] = state[\"question\"].content\n",
    "    return state\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "this node, \n",
    "this determines if the question is on topic or off topic, \n",
    "can't we simply skip this step ? so it will simply fetch documents from vector DB accordingly, but then grade it, if its not relevant it will try again..\n",
    "max 3 times then if not then it means it was a off topic question. \n",
    "thats a better workflow i believe? rather than hardcoding questions , if not one of them then off topic.\n",
    "\n",
    "Your suggestion has merit - instead of explicitly classifying questions as on-topic or off-topic upfront, we could:\n",
    "\n",
    "Always attempt to retrieve documents\n",
    "Grade the relevance of retrieved documents\n",
    "If no relevant documents are found after multiple attempts at question refinement, conclude the question was likely off-topic\n",
    "\n",
    "\n",
    "drawbacks:\n",
    "\n",
    "Efficiency: You'd always make at least one retrieval call, which costs time and potentially money if using paid vector storage.\n",
    "Precision: The explicit classifier could be more precise about certain topics that are clearly out of scope.\n",
    "User Experience: A user asking about completely unrelated topics might get a response like \"I couldn't find information about that\" rather than a more direct \"I can only answer questions about restaurant hours, prices, and ownership.\"\n",
    "Resource Usage: More database queries and potentially more LLM calls if you're constantly trying to refine irrelevant questions.\n",
    "\n",
    "Hybrid approach:\n",
    "\n",
    "def lightweight_classifier(state: AgentState):\n",
    "    \\\"\"\"A very simple classifier that only filters out obviously unrelated topics\\\"\"\"\n",
    "    question = state[\"rephrased_question\"].lower()\n",
    "    \n",
    "    # List of obviously off-topic keywords\n",
    "    off_topic_indicators = [\n",
    "        \"stock market\", \"weather\", \"sports scores\", \"politics\", \n",
    "        \"movie times\", \"crypto\", \"bitcoin\"\n",
    "    ]\n",
    "    \n",
    "    # Only reject if obviously off-topic\n",
    "    for topic in off_topic_indicators:\n",
    "        if topic in question:\n",
    "            state[\"off_topic\"] = True\n",
    "            return state\n",
    "    \n",
    "    # Otherwise, proceed to retrieval\n",
    "    state[\"off_topic\"] = False\n",
    "    return state\n",
    "\n",
    "def router_after_classification(state: AgentState):\n",
    "    if state[\"off_topic\"]:\n",
    "        return \"off_topic_response\"\n",
    "    return \"retrieve\"\n",
    "\n",
    "\"\"\"\n",
    "def question_classifier(state: AgentState):\n",
    "    print(\"Entering question_classifier\")\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a classifier that determines whether a user's question is about one of the following topics:\n",
    "\n",
    "    1. Information about the owner of Bella Vista, which is Antonio Rossi.\n",
    "    2. Prices of dishes at Bella Vista (restaurant).\n",
    "    3. Opening hours of Bella Vista (restaurant).\n",
    "\n",
    "    If the question IS about any of these topics, respond with 'Yes'. Otherwise, respond with 'No'.\"\"\"\n",
    "    )\n",
    "\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"User question: {state['rephrased_question']}\"\n",
    "    )\n",
    "    grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    \"\"\" structure pydanctic output which is: 'score': yes \"\"\"\n",
    "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    result = grader_llm.invoke({})\n",
    "    # strip() strips all the whitespaces around the string.\n",
    "    state[\"on_topic\"] = result.score.strip()\n",
    "    print(f\"question_classifier: on_topic = {state['on_topic']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def on_topic_router(state: AgentState):\n",
    "    print(\"Entering on_topic_router\")\n",
    "    on_topic = state.get(\"on_topic\", \"\").strip().lower()\n",
    "    if on_topic == \"yes\":\n",
    "        print(\"Routing to retrieve\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Routing to off_topic_response\")\n",
    "        return \"off_topic_response\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    print(\"Entering retrieve\")\n",
    "\n",
    "    \"\"\" \n",
    "    This might seem like it's skipping the embedding step, but here's what's actually happening behind the scenes:\n",
    "\n",
    "    When you call retriever.invoke(query), the retriever internally handles the embedding process. The embedding happens implicitly within the retriever's implementation, not explicitly in your code.\n",
    "    \"\"\"\n",
    "    \"\"\" \n",
    "    What's happening under the hood:\n",
    "\n",
    "    The retriever takes your text query (state[\"rephrased_question\"])\n",
    "    It passes this text to the embedding model that was configured when the vector store was created\n",
    "    The embedding model converts the text to a vector\n",
    "    The vector is used to perform similarity search against the vectors in the database\n",
    "    The matching documents are returned\n",
    "    \"\"\"\n",
    "\n",
    "    documents = retriever.invoke(state[\"rephrased_question\"])\n",
    "    print(f\"retrieve: Retrieved {len(documents)} documents\")\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "\n",
    "class GradeDocument(BaseModel):\n",
    "    score: str = Field(\n",
    "        description=\"Document is relevant to the question? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def retrieval_grader(state: AgentState):\n",
    "    print(\"Entering retrieval_grader\")\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a grader assessing the relevance of a retrieved document to a user question.\n",
    "                    Only answer with 'Yes' or 'No'.\n",
    "\n",
    "                    If the document contains information relevant to the user's question, respond with 'Yes'.\n",
    "                    Otherwise, respond with 'No'.\"\"\"\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    structured_llm = llm.with_structured_output(GradeDocument)\n",
    "\n",
    "    relevant_docs = []\n",
    "    for doc in state[\"documents\"]:\n",
    "        human_message = HumanMessage(\n",
    "            content=f\"User question: {state['rephrased_question']}\\n\\nRetrieved document:\\n{doc.page_content}\"\n",
    "        )\n",
    "        grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "        grader_llm = grade_prompt | structured_llm\n",
    "        result = grader_llm.invoke({})\n",
    "        print(\n",
    "            f\"Grading document: {doc.page_content[:30]}... Result: {result.score.strip()}\"\n",
    "        )\n",
    "        if result.score.strip().lower() == \"yes\":\n",
    "            relevant_docs.append(doc)\n",
    "    state[\"documents\"] = relevant_docs\n",
    "    state[\"proceed_to_generate\"] = len(relevant_docs) > 0\n",
    "    print(f\"retrieval_grader: proceed_to_generate = {state['proceed_to_generate']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def proceed_router(state: AgentState):\n",
    "    print(\"Entering proceed_router\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if state.get(\"proceed_to_generate\", False):\n",
    "        print(\"Routing to generate_answer\")\n",
    "        return \"generate_answer\"\n",
    "    elif rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached. Cannot find relevant documents.\")\n",
    "        return \"cannot_answer\"\n",
    "    else:\n",
    "        print(\"Routing to refine_question\")\n",
    "        return \"refine_question\"\n",
    "\n",
    "\n",
    "def refine_question(state: AgentState):\n",
    "    print(\"Entering refine_question\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached\")\n",
    "        return state\n",
    "    question_to_refine = state[\"rephrased_question\"]\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a helpful assistant that slightly refines the user's question to improve retrieval results.\n",
    "Provide a slightly adjusted version of the question.\"\"\"\n",
    "    )\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"Original question: {question_to_refine}\\n\\nProvide a slightly refined question.\"\n",
    "    )\n",
    "    refine_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    prompt = refine_prompt.format()\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "\n",
    "    refined_question = response.content.strip()\n",
    "    print(f\"refine_question: Refined question: {refined_question}\")\n",
    "    \n",
    "    state[\"rephrased_question\"] = refined_question\n",
    "    state[\"rephrase_count\"] = rephrase_count + 1\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_answer(state: AgentState):\n",
    "    print(\"Entering generate_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        raise ValueError(\"State must include 'messages' before generating an answer.\")\n",
    "\n",
    "    history = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    rephrased_question = state[\"rephrased_question\"]\n",
    "\n",
    "    response = rag_chain.invoke(\n",
    "        {\"history\": history, \"context\": documents, \"question\": rephrased_question}\n",
    "    )\n",
    "\n",
    "    generation = response.content.strip()\n",
    "\n",
    "    state[\"messages\"].append(AIMessage(content=generation))\n",
    "    print(f\"generate_answer: Generated response: {generation}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def cannot_answer(state: AgentState):\n",
    "    print(\"Entering cannot_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(\n",
    "            content=\"I'm sorry, but I cannot find the information you're looking for.\"\n",
    "        )\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    print(\"Entering off_topic_response\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(AIMessage(content=\"I can't respond to that!\"))\n",
    "    return state\n",
    "\n",
    "\n",
    "'''\n",
    "Instead, you could have the LLM generate contextually appropriate responses by:\n",
    "\n",
    "For off-topic questions: Have the LLM explain what topics it can assist with while politely declining the current question\n",
    "For no-relevant-documents scenarios: Have the LLM generate a response that acknowledges it couldn't find specific information\n",
    "\n",
    "Here's how you could refactor these nodes:\n",
    "\n",
    "def cannot_answer(state: AgentState):\n",
    "    print(\"Entering cannot_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    \n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a helpful assistant that specializes in answering questions about \n",
    "        restaurant Bella Vista. A user has asked a question, but after several attempts, \n",
    "        no relevant information could be found. Politely explain that you don't have the \n",
    "        specific information they're looking for, but mention what topics you can help with \n",
    "        (restaurant hours, menu prices, and owner information).\"\"\"\n",
    "    )\n",
    "    \n",
    "    human_message = HumanMessage(\n",
    "        content=f\"Question: {state['rephrased_question']}\"\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    response = llm.invoke([system_message, human_message])\n",
    "    \n",
    "    state[\"messages\"].append(response)\n",
    "    return state\n",
    "\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    print(\"Entering off_topic_response\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    \n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a helpful assistant that specializes in answering questions about \n",
    "        restaurant Bella Vista. The user has asked a question that is outside your area of \n",
    "        expertise. Politely explain that you can only answer questions about Bella Vista \n",
    "        restaurant, specifically about its opening hours, menu prices, and owner information.\"\"\"\n",
    "    )\n",
    "    \n",
    "    human_message = HumanMessage(\n",
    "        content=f\"Question: {state['rephrased_question']}\"\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    response = llm.invoke([system_message, human_message])\n",
    "    \n",
    "    state[\"messages\"].append(response)\n",
    "    return state\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"question_rewriter\", question_rewriter)\n",
    "workflow.add_node(\"question_classifier\", question_classifier)\n",
    "workflow.add_node(\"off_topic_response\", off_topic_response)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"retrieval_grader\", retrieval_grader)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"refine_question\", refine_question)\n",
    "workflow.add_node(\"cannot_answer\", cannot_answer)\n",
    "\n",
    "workflow.add_edge(\"question_rewriter\", \"question_classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"question_classifier\",\n",
    "    on_topic_router,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"off_topic_response\": \"off_topic_response\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"retrieval_grader\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieval_grader\",\n",
    "    proceed_router,\n",
    "    {\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "        \"refine_question\": \"refine_question\",\n",
    "        \"cannot_answer\": \"cannot_answer\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"refine_question\", \"retrieve\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"cannot_answer\", END)\n",
    "workflow.add_edge(\"off_topic_response\", END)\n",
    "workflow.set_entry_point(\"question_rewriter\")\n",
    "graph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\"question\": HumanMessage(content=\"How is the weather?\")}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No docs found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"question\": HumanMessage(\n",
    "        content=\"How old is the owner of the restaurant Bella Vista?\"\n",
    "    )\n",
    "}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 2}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rag with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"\"\" because of the pydantic state model, this question is auto stored in the state in 'question' key. \"\"\"\n",
    "    \"question\": HumanMessage(content=\"When does the Bella Vista restaurant open?\")\n",
    "}\n",
    "\n",
    "\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\"question\": HumanMessage(content=\"Also on sunday?\")}\n",
    "\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 3}})\n",
    "\n",
    "\"\"\" \n",
    "The critical part is this:\n",
    "\n",
    "When the question rewriter runs, it expects state[\"messages\"] to contain the conversation history\n",
    "But for the second invocation, your input only contains the new question: {\"question\": HumanMessage(content=\"Also on sunday?\")}\n",
    "Without MemorySaver, the state would be initialized fresh each time with just your input, so state[\"messages\"] would be empty or only contain the current question. so we wouldn't have anything in state['messages'] , so won't be able to provide the llm with the chat_history in both the nodes where we need send chat_history to the llm ( question_rewriter & generate )\n",
    "\n",
    "This is why you need MemorySaver: It persists the messages array between invocations, allowing your second invocation to access the conversation history from the first invocation. meaning it doesn't reset the state['messages'] after 2nd invocation.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
