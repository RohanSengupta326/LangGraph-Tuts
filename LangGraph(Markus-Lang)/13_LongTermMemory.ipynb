{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "InMemoryStore is a temporary storage that exists only during the program's execution\n",
    "A namespace is created by combining user_id and application_context to organize memory storage\n",
    "This namespace helps separate memories for different users and different applications\n",
    "\"\"\"\n",
    "store = InMemoryStore()\n",
    "user_id = \"my-user\"\n",
    "application_context = \"chitchat\"\n",
    "\n",
    "\"\"\"\n",
    "What is a Namespace?\n",
    "A namespace in LangGraph is a unique identifier used to group related memory items together. Technically, it's implemented as a tuple that serves as a prefix or \"address\" for storing and retrieving memory items.\n",
    "\"\"\"\n",
    "namespace = (user_id, application_context)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This saves two memory entries with the keys \"a-memory\" and \"another-memory\" under the specified namespace. Each entry contains structured data including rules about the user and a custom key-value pair.\n",
    "\"\"\"\n",
    "store.put(\n",
    "    namespace,\n",
    "    \"a-memory\",\n",
    "    {\n",
    "        \"rules\": [\n",
    "            \"User likes short, direct language\",\n",
    "            \"User only speaks English & python\",\n",
    "        ],\n",
    "        \"my-key\": \"my-value\",\n",
    "    },\n",
    ")\n",
    "store.put(\n",
    "    namespace,\n",
    "    \"another-memory\",\n",
    "    {\"rules\": [\"User prefers concise answers\"], \"my-key\": \"my-value\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can retrieve a specific memory:\n",
    "\"\"\"\n",
    "store.get(namespace, \"a-memory\").value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Or we can search for memories that match specific criteria: \"\"\"\n",
    "results = store.search(namespace, filter={\"my-key\": \"my-value\"})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in results:\n",
    "    print(item.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Literal\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.graph import StateGraph, MessagesState, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \"\"\"Get the weather at a specific location\"\"\"\n",
    "    if location.lower() in [\"munich\"]:\n",
    "        return \"It's 15 degrees Celsius and cloudy.\"\n",
    "    else:\n",
    "        return \"It's 32 degrees Celsius and sunny.\"\n",
    "\n",
    "\n",
    "tools = [get_weather]\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n",
    "store = InMemoryStore()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function is the heart of our memory system:\n",
    "\n",
    "It extracts the user_id from the config (with a default if none provided)\n",
    "Creates a namespace specifically for memories with this user\n",
    "Retrieves all existing memories for this user\n",
    "Combines them into a string to include in the system message\n",
    "Checks if the user's message contains \"remember\" to store new information\n",
    "If it does, it extracts the content after \"remember\" and stores it with a unique ID\n",
    "Finally, it calls the model with the complete context and returns the response\n",
    "\"\"\"\n",
    "def call_model(state: MessagesState, config: dict, *, store: BaseStore):\n",
    "    user_id = config.get(\"configurable\", {}).get(\"user_id\", \"default-user\")\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n",
    "    system_msg = \"You are a helpful assistant.\"\n",
    "    if info:\n",
    "        system_msg += f\" User info:\\n{info}\"\n",
    "    print(\"System Message:\", system_msg)\n",
    "\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"remember\" in last_message.content.lower():\n",
    "        memory_content = last_message.content.lower().split(\"remember\", 1)[1].strip()\n",
    "        if memory_content:\n",
    "            memory = memory_content\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n",
    "\n",
    "            \n",
    "    model_input_messages = [SystemMessage(content=system_msg)] + messages\n",
    "    response = model.invoke(model_input_messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = MemorySaver()\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "tool_node = ToolNode(tools)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "graph = workflow.compile(checkpointer=checkpointer, store=store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Information from across multiple threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "only using MemorySaver() would remember the chat history of only that thread_id. in other thread_id it won't remember the chat history. \n",
    "but with InMemoryStore it would remember based on the NameSpace inserted. in this case with namespace name: 'memories' & user_id.\n",
    "so even in different thread_id it would still remember if something it has to remember as long as the user_id is still same. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\" This stores \"my name is alice\" in store for user \"user123\". \"\"\"\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Remember my name is Alice.\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"user123\", \"thread_id\": 1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Even in a new thread (thread_id 2), but with the same user_id, the assistant remembers the name: \"\"\"\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name?\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"user123\", \"thread_id\": 2}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For a different user_id, there's no stored memory: \"\"\"\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name?\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"userxyz\", \"thread_id\": 3}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg import Connection\n",
    "from psycopg.rows import dict_row\n",
    "from langgraph.store.postgres import PostgresStore  # psycopg3 required!\n",
    "\n",
    "con_string = \"postgresql://postgres:postgres@localhost:5433/postgres\"\n",
    "\n",
    "conn = Connection.connect(\n",
    "    con_string, autocommit=True, prepare_threshold=0, row_factory=dict_row\n",
    ")\n",
    "\n",
    "postgres_store = PostgresStore(conn=conn)\n",
    "postgres_store.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We rebuild our graph with the PostgreSQL store instead of the in-memory one.\n",
    "instead\n",
    "\"\"\"\n",
    "graph = workflow.compile(checkpointer=checkpointer, store=postgres_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Remember my name is Alice.\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"user12345\", \"thread_id\": \"x\"}},\n",
    ")\n",
    "\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name?\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"user12345\", \"thread_id\": \"y\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
