{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \"\"\"Call to get the current weather.\"\"\"\n",
    "    if location.lower() in [\"munich\"]:\n",
    "        return \"It's 15 degrees Celsius and cloudy.\"\n",
    "    else:\n",
    "        return \"It's 32 degrees Celsius and sunny.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather]\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How is the weather in munich?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"What would you recommend to do in that city then?\")\n",
    "        ]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": 1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting production ready - async and streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This line configures the ChatOpenAI model with the streaming=True parameter. This is a critical difference from the earlier code. When streaming is enabled:\n",
    "\n",
    "The LLM will return chunks of the response as they're generated\n",
    "Users see the response being built word by word\n",
    "Your application can feel more responsive, especially for longer responses\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's what's happening:\n",
    "\n",
    "The function is now defined with async def - making it an asynchronous function\n",
    "Instead of model.invoke(), it uses await model.ainvoke(messages)\n",
    "It still returns the response in the same state format\n",
    "\n",
    "This change to asynchronous processing is important because:\n",
    "\n",
    "It allows your application to handle multiple requests simultaneously\n",
    "The server doesn't get blocked waiting for the LLM to respond\n",
    "Your application can remain responsive even under load\n",
    "\"\"\"\n",
    "\n",
    "async def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = await model.ainvoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "graph = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"How is the weather in Munich?\")]}\n",
    "config = {\"configurable\": {\"thread_id\": 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instead of graph.invoke(), this uses await graph.ainvoke() to call the graph asynchronously. This allows the operation to be awaited, making it work within an async context. This is essential when:\n",
    "\n",
    "Your application handles multiple users concurrently\n",
    "You're working in an async framework like FastAPI\n",
    "You need to perform other operations while waiting for LLM responses\n",
    "\"\"\"\n",
    "\n",
    "await graph.ainvoke(input=inputs, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This demonstrates streaming with stream_mode=\"updates\":\n",
    "\n",
    "graph.astream() returns an async iterator\n",
    "The stream_mode=\"updates\" parameter makes it yield entire updates from each node\n",
    "The code loops through each update as it arrives\n",
    "For each update, it prints information about which node produced it and the message content\n",
    "\n",
    "This is useful when you want to show users which part of your agent is currently working (e.g., \"Agent thinking...\" followed by \"Tool executing...\").\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This code with stream_mode=\"updates\" shows complete updates from each node in the graph. It doesn't show token-by-token generation, but rather complete outputs when each node in your graph finishes its work.\n",
    "What you'll see is something like:\n",
    "\n",
    "\"Output from node 'agent':\" - followed by the complete LLM response\n",
    "\"Output from node 'tools':\" - followed by the complete tool response (if a tool was called)\n",
    "\"Output from node 'agent':\" again - if the LLM responds to the tool output\n",
    "\n",
    "Each node outputs its complete result before moving to the next node. This is useful for tracking the workflow through your graph, showing which component is active at each stage.\n",
    "\"\"\"\n",
    "inputs = {\"messages\": [HumanMessage(content=\"How is the weather in Munich?\")]}\n",
    "async for output in graph.astream(inputs, stream_mode=\"updates\", config=config):\n",
    "    # stream_mode=\"updates\" yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value[\"messages\"][-1].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here's what's happening in this code:\n",
    "\n",
    "This code streams the AI's response token by token. When we say \"token\", it's usually small pieces of text (might be parts of words, punctuation, etc.).\n",
    "The graph.astream() with stream_mode=\"messages\" yields each small piece of the AI's response as it's being generated.\n",
    "Now, let's look at the two conditions inside the loop:\n",
    "\n",
    "First condition: if msg.content and not isinstance(msg, HumanMessage):\n",
    "\n",
    "This checks if the message has content AND is NOT a human message\n",
    "For any message that meets these criteria, it prints the content followed by a pipe symbol (|)\n",
    "This is just visualizing each chunk as it comes in (giving a \"typewriter effect\")\n",
    "\n",
    "\n",
    "Second condition: if isinstance(msg, AIMessageChunk):\n",
    "\n",
    "This specifically checks if the message is an AI message chunk\n",
    "If it's the first chunk (gathered is None), it initializes gathered with this chunk\n",
    "If we already have gathered chunks, it adds this new chunk to them using the + operator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "After the loop completes, print(gathered.content) displays the entire reassembled message.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "What's happening in the code:\n",
    "\n",
    "First condition (if msg.content and not isinstance(msg, HumanMessage):):\n",
    "\n",
    "This catches ALL non-human messages that have content\n",
    "This includes AIMessageChunk and potentially other message types\n",
    "It prints each chunk's content with a pipe symbol\n",
    "This gives you a visual of the chunks arriving in real-time\n",
    "\n",
    "\n",
    "Second condition (if isinstance(msg, AIMessageChunk):):\n",
    "\n",
    "This specifically targets only AIMessageChunk objects\n",
    "It builds the complete message by combining all chunks\n",
    "The first chunk initializes gathered\n",
    "Each subsequent chunk is added to gathered using the + operator\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.messages import AIMessageChunk, HumanMessage\n",
    "\n",
    "inputs = [HumanMessage(content=\"How is the weather in Munich?\")]\n",
    "gathered = None\n",
    "\n",
    "async for msg, metadata in graph.astream(\n",
    "    {\"messages\": inputs}, stream_mode=\"messages\", config=config\n",
    "):\n",
    "    if msg.content and not isinstance(msg, HumanMessage):\n",
    "        # Print each token as it streams in\n",
    "        print(msg.content, end=\"|\", flush=True)\n",
    "\n",
    "    # Handle the AI message chunks for proper assembly\n",
    "    if isinstance(msg, AIMessageChunk):\n",
    "        if gathered is None:\n",
    "            gathered = msg\n",
    "        else:\n",
    "            gathered = gathered + msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gathered.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
